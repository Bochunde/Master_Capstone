{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.benchmarks import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190  Python-3.8.16 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3006818 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100%|██████████| 914/914 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\images\\3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 58/58 [00:04<00:00, 12.68it/s]\n",
      "                   all        914       3227      0.622      0.465      0.492      0.324\n",
      "                 Apple        914        557      0.586      0.476      0.512      0.362\n",
      "                Banana        914        390      0.628      0.463      0.483      0.277\n",
      "                 Grape        914        809      0.566      0.391       0.39      0.248\n",
      "                Orange        914       1100        0.6      0.415      0.432      0.272\n",
      "             Pineapple        914        154      0.703      0.461      0.523      0.332\n",
      "            Watermelon        914        217      0.648      0.584      0.611      0.454\n",
      "Speed: 0.3ms preprocess, 1.0ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002A6A1F84E20>\n",
       "fitness: 0.3408502217810829\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.36169,     0.27682,     0.24775,     0.27247,     0.33202,     0.45351])\n",
       "names: {0: 'Apple', 1: 'Banana', 2: 'Grape', 3: 'Orange', 4: 'Pineapple', 5: 'Watermelon'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.6218931202481613, 'metrics/recall(B)': 0.4649298110506978, 'metrics/mAP50(B)': 0.4921184481967759, 'metrics/mAP50-95(B)': 0.32404264106822805, 'fitness': 0.3408502217810829}\n",
       "save_dir: WindowsPath('runs/detect/val2')\n",
       "speed: {'preprocess': 0.3449350791113121, 'inference': 0.9973769114888732, 'loss': 0.0, 'postprocess': 0.6936115561220265}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('best_train36.pt')\n",
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190  Python-3.8.16 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100%|██████████| 914/914 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\images\\3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 58/58 [00:03<00:00, 15.20it/s]\n",
      "                   all        914       3227     0.0102     0.0229    0.00544    0.00317\n",
      "                person        914        557    0.00659      0.104    0.00368    0.00239\n",
      "               bicycle        914        390     0.0344     0.0231     0.0181     0.0136\n",
      "                   car        914        809    0.00459    0.00494    0.00232    0.00169\n",
      "            motorcycle        914       1100    0.00752   0.000909    0.00377   0.000377\n",
      "              airplane        914        154          0          0          0          0\n",
      "                   bus        914        217    0.00833    0.00461    0.00477   0.000954\n",
      "Speed: 0.2ms preprocess, 0.9ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3, 4, 5])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002A6FE782040>\n",
       "fitness: 0.0033930977127851303\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([  0.0023882,    0.013587,   0.0016861,  0.00037662,           0,  0.00095382,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,\n",
       "         0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,\n",
       "         0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,\n",
       "         0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653,   0.0031653])\n",
       "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.010230598014211818, 'metrics/recall(B)': 0.022944658100545395, 'metrics/mAP50(B)': 0.005443342130751101, 'metrics/mAP50-95(B)': 0.003165292777455578, 'fitness': 0.0033930977127851303}\n",
       "save_dir: WindowsPath('runs/detect/val3')\n",
       "speed: {'preprocess': 0.24819374084472656, 'inference': 0.8661089632130295, 'loss': 0.0, 'postprocess': 0.7257573714141512}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "model.val(data='second_dataset/archive/Fruits-detection/data.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 87.2ms\n",
      "Speed: 4.0ms preprocess, 87.2ms inference, 44.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100%|██████████| 914/914 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\images\\3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 914/914 [00:10<00:00, 83.66it/s]\n",
      "                   all        914       3227     0.0103     0.0232    0.00546    0.00317\n",
      "Speed: 0.4ms preprocess, 4.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.0.0...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success  0.9s, saved as 'yolov8n.torchscript' (12.5 MB)\n",
      "\n",
      "Export complete (0.9s)\n",
      "Results saved to \u001b[1mC:\\Users\\97659\\Desktop\\homework\\grad 2023 FALL\\Capstone\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Loading yolov8n.torchscript for TorchScript inference...\n",
      "\n",
      "image 1/1 C:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x640 4 persons, 1 bus, 7.0ms\n",
      "Speed: 8.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Loading yolov8n.torchscript for TorchScript inference...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100%|██████████| 914/914 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\images\\3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 914/914 [00:10<00:00, 87.27it/s]\n",
      "                   all        914       3227    0.00756     0.0232    0.00401    0.00195\n",
      "Speed: 0.5ms preprocess, 3.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.5s, saved as 'yolov8n.onnx' (12.2 MB)\n",
      "\n",
      "Export complete (0.5s)\n",
      "Results saved to \u001b[1mC:\\Users\\97659\\Desktop\\homework\\grad 2023 FALL\\Capstone\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Loading yolov8n.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 C:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x640 4 persons, 1 bus, 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Loading yolov8n.onnx for ONNX Runtime inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forcing batch=1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100%|██████████| 914/914 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\images\\3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 914/914 [00:34<00:00, 26.88it/s]\n",
      "                   all        914       3227    0.00757     0.0232    0.00401    0.00196\n",
      "Speed: 1.2ms preprocess, 26.6ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "ERROR  Benchmark failure for OpenVINO: inference not supported on GPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure  0.0s: No module named 'tensorrt'\n",
      "ERROR  Benchmark failure for TensorRT: No module named 'tensorrt'\n",
      "ERROR  Benchmark failure for CoreML: inference not supported on GPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.13.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.35...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.1s, saved as 'yolov8n.onnx' (12.2 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m running 'onnx2tf -i \"yolov8n.onnx\" -o \"yolov8n_saved_model\" -nuo --non_verbose'\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success  12.6s, saved as 'yolov8n_saved_model' (30.6 MB)\n",
      "\n",
      "Export complete (12.7s)\n",
      "Results saved to \u001b[1mC:\\Users\\97659\\Desktop\\homework\\grad 2023 FALL\\Capstone\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n_saved_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n_saved_model imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Loading yolov8n_saved_model for TensorFlow SavedModel inference...\n",
      "\n",
      "image 1/1 C:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x640 4 persons, 1 bus, 196.3ms\n",
      "Speed: 2.0ms preprocess, 196.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Loading yolov8n_saved_model for TensorFlow SavedModel inference...\n",
      "Forcing batch=1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100%|██████████| 914/914 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\97659\\Desktop\\homework\\grad_2023_SPRING\\CSE_573\\final_project\\second_dataset\\archive\\Fruits-detection\\valid\\images\\3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  83%|████████▎ | 757/914 [01:10<00:14, 10.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\97659\\Desktop\\homework\\grad 2023 FALL\\Capstone\\plotting.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/97659/Desktop/homework/grad%202023%20FALL/Capstone/plotting.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m yolov8n_on_dataset \u001b[39m=\u001b[39m benchmark(model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39myolov8n.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msecond_dataset/archive/Fruits-detection/data.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, imgsz\u001b[39m=\u001b[39;49m\u001b[39m640\u001b[39;49m, half\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\lib\\site-packages\\ultralytics\\utils\\benchmarks.py:119\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m(model, data, imgsz, half, int8, device, verbose)\u001b[0m\n\u001b[0;32m    117\u001b[0m data \u001b[39m=\u001b[39m data \u001b[39mor\u001b[39;00m TASK2DATA[model\u001b[39m.\u001b[39mtask]  \u001b[39m# task to dataset, i.e. coco8.yaml for task=detect\u001b[39;00m\n\u001b[0;32m    118\u001b[0m key \u001b[39m=\u001b[39m TASK2METRIC[model\u001b[39m.\u001b[39mtask]  \u001b[39m# task to metric, i.e. metrics/mAP50-95(B) for task=detect\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m results \u001b[39m=\u001b[39m exported_model\u001b[39m.\u001b[39;49mval(data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    120\u001b[0m                              batch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    121\u001b[0m                              imgsz\u001b[39m=\u001b[39;49mimgsz,\n\u001b[0;32m    122\u001b[0m                              plots\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    123\u001b[0m                              device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    124\u001b[0m                              half\u001b[39m=\u001b[39;49mhalf,\n\u001b[0;32m    125\u001b[0m                              int8\u001b[39m=\u001b[39;49mint8,\n\u001b[0;32m    126\u001b[0m                              verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    127\u001b[0m metric, speed \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mresults_dict[key], results\u001b[39m.\u001b[39mspeed[\u001b[39m'\u001b[39m\u001b[39minference\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    128\u001b[0m y\u001b[39m.\u001b[39mappend([name, \u001b[39m'\u001b[39m\u001b[39m✅\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mround\u001b[39m(file_size(filename), \u001b[39m1\u001b[39m), \u001b[39mround\u001b[39m(metric, \u001b[39m4\u001b[39m), \u001b[39mround\u001b[39m(speed, \u001b[39m2\u001b[39m)])\n",
      "File \u001b[1;32mc:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\lib\\site-packages\\ultralytics\\engine\\model.py:269\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcustom, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m}  \u001b[39m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    268\u001b[0m validator \u001b[39m=\u001b[39m (validator \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_smart_load(\u001b[39m'\u001b[39m\u001b[39mvalidator\u001b[39m\u001b[39m'\u001b[39m))(args\u001b[39m=\u001b[39margs, _callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m--> 269\u001b[0m validator(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[0;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m validator\u001b[39m.\u001b[39mmetrics\n\u001b[0;32m    271\u001b[0m \u001b[39mreturn\u001b[39;00m validator\u001b[39m.\u001b[39mmetrics\n",
      "File \u001b[1;32mc:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\lib\\site-packages\\ultralytics\\engine\\validator.py:178\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39mwith\u001b[39;00m dt[\u001b[39m3\u001b[39m]:\n\u001b[1;32m--> 178\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpostprocess(preds)\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_metrics(preds, batch)\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mplots \u001b[39mand\u001b[39;00m batch_i \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:82\u001b[0m, in \u001b[0;36mDetectionValidator.postprocess\u001b[1;34m(self, preds)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpostprocess\u001b[39m(\u001b[39mself\u001b[39m, preds):\n\u001b[0;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mnon_max_suppression(preds,\n\u001b[0;32m     83\u001b[0m                                    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mconf,\n\u001b[0;32m     84\u001b[0m                                    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49miou,\n\u001b[0;32m     85\u001b[0m                                    labels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlb,\n\u001b[0;32m     86\u001b[0m                                    multi_label\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     87\u001b[0m                                    agnostic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msingle_cls,\n\u001b[0;32m     88\u001b[0m                                    max_det\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mmax_det)\n",
      "File \u001b[1;32mc:\\Users\\97659\\Anaconda3\\envs\\conda_gpu\\lib\\site-packages\\ultralytics\\utils\\ops.py:240\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh)\u001b[0m\n\u001b[0;32m    237\u001b[0m     x \u001b[39m=\u001b[39m x[x[:, \u001b[39m4\u001b[39m]\u001b[39m.\u001b[39margsort(descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:max_nms]]  \u001b[39m# sort by confidence and remove excess boxes\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m# Batched NMS\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m c \u001b[39m=\u001b[39m x[:, \u001b[39m5\u001b[39;49m:\u001b[39m6\u001b[39;49m] \u001b[39m*\u001b[39;49m (\u001b[39m0\u001b[39;49m \u001b[39mif\u001b[39;49;00m agnostic \u001b[39melse\u001b[39;49;00m max_wh)  \u001b[39m# classes\u001b[39;00m\n\u001b[0;32m    241\u001b[0m boxes, scores \u001b[39m=\u001b[39m x[:, :\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m c, x[:, \u001b[39m4\u001b[39m]  \u001b[39m# boxes (offset by class), scores\u001b[39;00m\n\u001b[0;32m    242\u001b[0m i \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mnms(boxes, scores, iou_thres)  \u001b[39m# NMS\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yolov8n_on_dataset = benchmark(model='yolov8n.pt', data='second_dataset/archive/Fruits-detection/data.yaml', imgsz=640, half=False, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
